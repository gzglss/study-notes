{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer,BertModel\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:11:22.779030Z","iopub.execute_input":"2022-12-04T02:11:22.779487Z","iopub.status.idle":"2022-12-04T02:11:25.314899Z","shell.execute_reply.started":"2022-12-04T02:11:22.779397Z","shell.execute_reply":"2022-12-04T02:11:25.313542Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:11:27.747768Z","iopub.execute_input":"2022-12-04T07:11:27.748172Z","iopub.status.idle":"2022-12-04T07:11:27.753983Z","shell.execute_reply.started":"2022-12-04T07:11:27.748135Z","shell.execute_reply":"2022-12-04T07:11:27.752730Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# data_path=\"/kaggle/input/bus-emotion-gz/20188_new.csv\"\ndata_path=\"/kaggle/input/bus-emo-900/bus_emo_900.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:11:35.300439Z","iopub.execute_input":"2022-12-04T02:11:35.300835Z","iopub.status.idle":"2022-12-04T02:11:35.305872Z","shell.execute_reply.started":"2022-12-04T02:11:35.300800Z","shell.execute_reply":"2022-12-04T02:11:35.304555Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# texts,labels=[],[]\n# with open(data_path,'r',encoding='gbk',errors='ignore') as f:\n#     while True:\n#         line=f.readline()\n#         if line:\n#             line=line.strip().split(',')\n#             if len(line[-2])==0:\n#                 break\n#             texts.append(line[-2])\n#             labels.append(line[-1])\n#         else:\n#             break\n# dic={}\n# dic['text'],dic['label']=texts,labels\n# df=pd.DataFrame(dic)\n\n# #数据去重\n# df=df.drop_duplicates()\n\n# train_df,valid_df=train_test_split(df,test_size=0.2,random_state=1)\n\n# #需要对数据的长度进行处理，长度太过参差\n# #工业上一般短文本和长文本使用不同的判断手段\n# df['text'].apply(len).describe()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T11:56:09.439085Z","iopub.execute_input":"2022-11-27T11:56:09.439755Z","iopub.status.idle":"2022-11-27T11:56:09.494044Z","shell.execute_reply.started":"2022-11-27T11:56:09.439719Z","shell.execute_reply":"2022-11-27T11:56:09.493155Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# df.to_csv(\"./bus_emo.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T12:44:40.093385Z","iopub.execute_input":"2022-11-25T12:44:40.093791Z","iopub.status.idle":"2022-11-25T12:44:40.138025Z","shell.execute_reply.started":"2022-11-25T12:44:40.093761Z","shell.execute_reply":"2022-11-25T12:44:40.136798Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# df.groupby('label').count()","metadata":{"execution":{"iopub.status.busy":"2022-11-26T02:46:11.643348Z","iopub.execute_input":"2022-11-26T02:46:11.643700Z","iopub.status.idle":"2022-11-26T02:46:11.670228Z","shell.execute_reply.started":"2022-11-26T02:46:11.643671Z","shell.execute_reply":"2022-11-26T02:46:11.669246Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"       text\nlabel      \n0      5188\n1      3168","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5188</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3168</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=pd.read_csv(data_path,encoding='gbk')\n#数据去重\ndf=df.drop_duplicates()\ntrain_df,valid_df=train_test_split(df,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:11:45.203636Z","iopub.execute_input":"2022-12-04T02:11:45.204037Z","iopub.status.idle":"2022-12-04T02:11:45.245808Z","shell.execute_reply.started":"2022-12-04T02:11:45.204002Z","shell.execute_reply":"2022-12-04T02:11:45.244914Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"数据清理：\n\n- 水话清除\n  - #A#\n  - @\n\n- 括号转换：【A】->A，...","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-12-03T13:59:33.658966Z","iopub.execute_input":"2022-12-03T13:59:33.659878Z","iopub.status.idle":"2022-12-03T13:59:33.689630Z","shell.execute_reply.started":"2022-12-03T13:59:33.659828Z","shell.execute_reply":"2022-12-03T13:59:33.687651Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                  text  label\n0                                             广州公交，给力！      1\n1                   社会主义式垃圾袋——蚊帐广州有些新公交座椅旁边有隔板可以放东西，贴心      1\n2          //@NLTV爆炸头小子:喜大普奔，中山市三角镇至广州南沙自贸区公交专线今日正式开通。      1\n3         喜大普奔，中山三角镇至广州南沙自贸区公交专线今日正式开通！@葛雨晴-miyoyo@鞠婧祎      1\n4                    司机师傅一边开着新公交车一边满脸自豪地跟乘客们介绍站点和地铁口?\"      1\n..                                                 ...    ...\n894  看到广州现在上下班高峰的人流，无论是在挤地铁还是在挤公交，真的毫无幸福感可言，那个人流密集度...      0\n895  每天在奥林匹克中心等325和高峰63的人那么多，为什么就不能多安排几辆车呢？@广州公交集团每...      0\n896  plk233近日，广州78路公交，司机关闭车门启动车辆后，一名女乘客发现自己坐过站，要求马上...      0\n897  我第一次用公交车乘车码是在广州，不过早期地铁比较麻烦，就没有用，有一点体验不好，无法接受，马...      1\n898                   不去远的地方都搭公交咯，玩这些？？上次去广州南排了半小时进站。。      0\n\n[899 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>广州公交，给力！</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>社会主义式垃圾袋——蚊帐广州有些新公交座椅旁边有隔板可以放东西，贴心</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>//@NLTV爆炸头小子:喜大普奔，中山市三角镇至广州南沙自贸区公交专线今日正式开通。</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>喜大普奔，中山三角镇至广州南沙自贸区公交专线今日正式开通！@葛雨晴-miyoyo@鞠婧祎</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>司机师傅一边开着新公交车一边满脸自豪地跟乘客们介绍站点和地铁口?\"</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>看到广州现在上下班高峰的人流，无论是在挤地铁还是在挤公交，真的毫无幸福感可言，那个人流密集度...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>每天在奥林匹克中心等325和高峰63的人那么多，为什么就不能多安排几辆车呢？@广州公交集团每...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>plk233近日，广州78路公交，司机关闭车门启动车辆后，一名女乘客发现自己坐过站，要求马上...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>897</th>\n      <td>我第一次用公交车乘车码是在广州，不过早期地铁比较麻烦，就没有用，有一点体验不好，无法接受，马...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>不去远的地方都搭公交咯，玩这些？？上次去广州南排了半小时进站。。</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>899 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def char_change(text):\n    if \"【\" in text:\n        lst=list(text)\n        s=lst.index('【')\n        e=0\n        if '】' in text:\n            e=lst.index('】')\n        if e==0:\n            lst.remove('【')\n        else:\n            if s==0:\n                lst[e]='。'\n                lst.remove('【')\n            else:\n                lst[s]='。'\n                lst[e]='。'\n        return ''.join(lst)\n    elif \"】\" in text:\n        lst=list(text)\n        lst.remove('】')\n        return ''.join(lst)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:11:52.080390Z","iopub.execute_input":"2022-12-04T02:11:52.080771Z","iopub.status.idle":"2022-12-04T02:11:52.088312Z","shell.execute_reply.started":"2022-12-04T02:11:52.080737Z","shell.execute_reply":"2022-12-04T02:11:52.087073Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def clean_suffix(text):\n    #主要用来清除@和#后面的文字\n    chars=\":：!！?？,，.。@#（）()<>《》“”\"\";；\"\n    lst=list(text)\n    while '@' in lst:\n        idx=lst.index('@')\n        i=idx+1\n        e=-1\n        while i<len(lst):\n            if lst[i] in chars:\n                e=i-1\n                break\n            if i==len(lst)-1:\n                e=i\n            i+=1\n        if e>=0:\n            tmp=lst[:idx][:]+lst[e+1:][:]\n            lst=tmp[:]\n        else:\n            break\n    if len(lst)==0:\n        return text\n    text=''.join(lst)\n    if '#' in text:\n        pattern=re.compile('\\#.*\\#')\n        text=re.sub(pattern,'',text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:11:54.491734Z","iopub.execute_input":"2022-12-04T02:11:54.492683Z","iopub.status.idle":"2022-12-04T02:11:54.500771Z","shell.execute_reply.started":"2022-12-04T02:11:54.492643Z","shell.execute_reply":"2022-12-04T02:11:54.499745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def clean_url(text):\n    sentences = text.split(' ')\n    # 处理http://类链接\n    url_pattern = re.compile(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b', re.S)\n    # 处理无http://类链接\n    domain_pattern = re.compile(r'(\\b)*(.*?)\\.(com|cn)')\n    if len(sentences) > 0:\n        result = []\n        for item in sentences:\n            text = re.sub(url_pattern, '', item)\n            text = re.sub(domain_pattern,'', text)\n            result.append(text)\n        return ' '.join(result)\n    else:\n        return re.sub(url_pattern, '', sentences)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:11:57.363843Z","iopub.execute_input":"2022-12-04T02:11:57.364575Z","iopub.status.idle":"2022-12-04T02:11:57.371223Z","shell.execute_reply.started":"2022-12-04T02:11:57.364539Z","shell.execute_reply":"2022-12-04T02:11:57.369992Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def clean_html(text):\n    html_pattern = re.compile('</?\\w+[^>]*>', re.S)\n    text=re.sub(html_pattern,'', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:01.376871Z","iopub.execute_input":"2022-12-04T02:12:01.377474Z","iopub.status.idle":"2022-12-04T02:12:01.382630Z","shell.execute_reply.started":"2022-12-04T02:12:01.377440Z","shell.execute_reply":"2022-12-04T02:12:01.381545Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def clean_tag(text):\n    tag_pattern = re.compile('(\\[|\\#|【)(.*?)(\\#|\\]|\\】)', re.S)\n    text = re.sub(tag_pattern, '', text)\n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:26.670182Z","iopub.execute_input":"2022-12-04T02:12:26.670556Z","iopub.status.idle":"2022-12-04T02:12:26.675933Z","shell.execute_reply.started":"2022-12-04T02:12:26.670524Z","shell.execute_reply":"2022-12-04T02:12:26.674753Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def clean_at(text):\n    #暂时不用，因为有的@出现在句中或靠前，此时就会将后面的所有字都去除\n    at_pattern = re.compile('@\\S*', re.S)\n    text = re.sub(at_pattern, '', text)\n    return text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:29.508242Z","iopub.execute_input":"2022-12-04T02:12:29.508631Z","iopub.status.idle":"2022-12-04T02:12:29.514179Z","shell.execute_reply.started":"2022-12-04T02:12:29.508595Z","shell.execute_reply":"2022-12-04T02:12:29.512927Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def clean_nan(text,label):\n    new_text,new_label=[],[]\n    for i in range(len(label)):\n        text[i]=text[i].strip()\n        if text[i]!='':\n            new_text.append(text[i])\n            new_label.append(label[i])\n    return new_text,new_label","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:32.683293Z","iopub.execute_input":"2022-12-04T02:12:32.683761Z","iopub.status.idle":"2022-12-04T02:12:32.691443Z","shell.execute_reply.started":"2022-12-04T02:12:32.683713Z","shell.execute_reply":"2022-12-04T02:12:32.690436Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"texts_df=train_df['text'].progress_apply(char_change)\ntexts_df=texts_df.progress_apply(clean_suffix)\ntexts_df=texts_df.progress_apply(clean_url)\ntexts_df=texts_df.progress_apply(clean_html)\nlabels_df=train_df['label']","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:35.283845Z","iopub.execute_input":"2022-12-04T02:12:35.284293Z","iopub.status.idle":"2022-12-04T02:12:35.450289Z","shell.execute_reply.started":"2022-12-04T02:12:35.284260Z","shell.execute_reply":"2022-12-04T02:12:35.449280Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 719/719 [00:00<00:00, 149137.26it/s]\n100%|██████████| 719/719 [00:00<00:00, 72574.89it/s]\n100%|██████████| 719/719 [00:00<00:00, 5959.86it/s]\n100%|██████████| 719/719 [00:00<00:00, 139933.39it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_texts,train_labels=clean_nan(texts_df.values,labels_df.values)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:37.740654Z","iopub.execute_input":"2022-12-04T02:12:37.741440Z","iopub.status.idle":"2022-12-04T02:12:37.747656Z","shell.execute_reply.started":"2022-12-04T02:12:37.741395Z","shell.execute_reply":"2022-12-04T02:12:37.746722Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_texts=valid_df['text'].apply(char_change)\ntest_texts=test_texts.progress_apply(clean_suffix)\ntest_texts=test_texts.progress_apply(clean_url)\ntest_texts=test_texts.progress_apply(clean_html)\ntest_labels=valid_df['label']","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:39.935219Z","iopub.execute_input":"2022-12-04T02:12:39.935592Z","iopub.status.idle":"2022-12-04T02:12:39.984235Z","shell.execute_reply.started":"2022-12-04T02:12:39.935560Z","shell.execute_reply":"2022-12-04T02:12:39.983185Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 180/180 [00:00<00:00, 45645.39it/s]\n100%|██████████| 180/180 [00:00<00:00, 7289.65it/s]\n100%|██████████| 180/180 [00:00<00:00, 91779.08it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"test_texts,test_labels=clean_nan(test_texts.values,test_labels.values)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:42.708385Z","iopub.execute_input":"2022-12-04T02:12:42.708765Z","iopub.status.idle":"2022-12-04T02:12:42.715213Z","shell.execute_reply.started":"2022-12-04T02:12:42.708730Z","shell.execute_reply":"2022-12-04T02:12:42.714163Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"tokenizer=BertTokenizer.from_pretrained(\"bert-base-chinese\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:12:45.172104Z","iopub.execute_input":"2022-12-04T02:12:45.172544Z","iopub.status.idle":"2022-12-04T02:12:46.190684Z","shell.execute_reply.started":"2022-12-04T02:12:45.172505Z","shell.execute_reply":"2022-12-04T02:12:46.189626Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8017326f4e42beb4f21f79cbc729dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6caf7285ef2a4425a21bbbf3902a4921"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de5b57b62f3a442ea598be9dcf62a93f"}},"metadata":{}}]},{"cell_type":"code","source":"class Bert(nn.Module):\n    def __init__(self,num_class,emb_size,max_length):\n        super(Bert,self).__init__()\n        self.max_length=max_length\n        self.bert=BertModel.from_pretrained(\"bert-base-chinese\")\n        self.dropout=nn.Dropout(0.3)\n        self.linear=nn.Linear(emb_size,num_class)\n    \n    def forward(self,inputs):\n        data_token=tokenizer.batch_encode_plus(inputs,\n                                               padding=True,\n                                               truncation=True,\n                                               max_length=self.max_length)\n        \n        input_ids=torch.tensor(data_token[\"input_ids\"]).to(device)\n        attention_mask=torch.tensor(data_token[\"attention_mask\"]).to(device)\n        token_type_ids=torch.tensor(data_token[\"token_type_ids\"]).to(device)\n        encode=self.bert(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n        output=self.dropout(encode[0][:,0,:])\n        output=self.linear(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:13:02.836746Z","iopub.execute_input":"2022-12-04T02:13:02.837478Z","iopub.status.idle":"2022-12-04T02:13:02.845657Z","shell.execute_reply.started":"2022-12-04T02:13:02.837434Z","shell.execute_reply":"2022-12-04T02:13:02.844570Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class BertLstm(nn.Module):\n    def __init__(self,num_class,emb_size,max_length,unit):\n        super(BertLstm,self).__init__()\n        self.max_length=max_length\n        self.bert=BertModel.from_pretrained(\"bert-base-chinese\")\n        self.bilstm=nn.LSTM(input_size=emb_size,hidden_size=unit,bidirectional=True)\n        self.dropout=nn.Dropout(0.3)\n        self.linear=nn.Linear(unit*2,num_class)\n    \n    def forward(self,inputs):\n        data_token=tokenizer.batch_encode_plus(inputs,\n                                               padding=True,\n                                               truncation=True,\n                                               max_length=self.max_length)\n        \n        input_ids=torch.tensor(data_token[\"input_ids\"]).to(device)\n        attention_mask=torch.tensor(data_token[\"attention_mask\"]).to(device)\n        token_type_ids=torch.tensor(data_token[\"token_type_ids\"]).to(device)\n        encode=self.bert(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n        output=self.bilstm(encode[0][:,0,:])\n        output=self.dropout(output[0])\n        output=self.linear(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:51:43.746928Z","iopub.execute_input":"2022-12-04T07:51:43.747941Z","iopub.status.idle":"2022-12-04T07:51:43.756947Z","shell.execute_reply.started":"2022-12-04T07:51:43.747893Z","shell.execute_reply":"2022-12-04T07:51:43.755926Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class TextCNN(nn.Module):\n    def __init__(self,filter_sizes,emb_size,in_channels,num_filter):\n        super(TextCNN,self).__init__()\n        self.convlist=nn.ModuleList([nn.Conv2d(in_channels=in_channels,\n                                 out_channels=num_filter,\n                                 kernel_size=[filter_size,emb_size]) for filter_size in filter_sizes])\n        \n    def forward(self,inputs):\n        encodes=[]\n        inputs=inputs.unsqueeze(1)\n        for conv in self.convlist:\n            output=conv(inputs)\n            output=F.relu(output)\n            output=output.squeeze(3)\n            output=F.max_pool1d(output, output.shape[2]).squeeze(2)\n            encodes.append(output)\n        encode=torch.concat(encodes,dim=-1)\n        return encode","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:55:18.524046Z","iopub.execute_input":"2022-12-04T07:55:18.524615Z","iopub.status.idle":"2022-12-04T07:55:18.538542Z","shell.execute_reply.started":"2022-12-04T07:55:18.524583Z","shell.execute_reply":"2022-12-04T07:55:18.537221Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"class bert_textcnn(nn.Module):\n    def __init__(self,num_filter,filter_sizes,in_channels,num_class,emb_size,max_length):\n        super(bert_textcnn,self).__init__()\n        self.max_length=max_length\n        self.bert=BertModel.from_pretrained(\"bert-base-chinese\")\n        self.textcnn=TextCNN(filter_sizes=filter_sizes,\n                             emb_size=emb_size,\n                             in_channels=in_channels,\n                             num_filter=num_filter)\n        self.textcnn.to(device)\n        self.dropout=nn.Dropout(0.3)\n        self.linear=nn.Linear(num_filter*len(filter_sizes),num_class)\n    \n    def forward(self,inputs):\n        token_dic=tokenizer.batch_encode_plus(inputs,\n                                                padding=True,\n                                                truncation=True,\n                                                max_length=self.max_length)\n        input_ids=torch.tensor(token_dic[\"input_ids\"]).to(device)\n        attention_mask=torch.tensor(token_dic[\"attention_mask\"]).to(device)\n        token_type_ids=torch.tensor(token_dic[\"token_type_ids\"]).to(device)\n        bert_output=self.bert(input_ids=input_ids,\n                              attention_mask=attention_mask,\n                              token_type_ids=token_type_ids)\n        last_hidden_out=bert_output[0][:,:,:]\n        textcnn_output=self.textcnn(last_hidden_out)\n        output=self.dropout(textcnn_output)\n        output=self.linear(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:55:21.936522Z","iopub.execute_input":"2022-12-04T07:55:21.936881Z","iopub.status.idle":"2022-12-04T07:55:21.946654Z","shell.execute_reply.started":"2022-12-04T07:55:21.936849Z","shell.execute_reply":"2022-12-04T07:55:21.945439Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"#bert-base\ndevice=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert=Bert(2,768,128)\nbert.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:35:02.639695Z","iopub.execute_input":"2022-12-04T02:35:02.640076Z","iopub.status.idle":"2022-12-04T02:35:04.169037Z","shell.execute_reply.started":"2022-12-04T02:35:02.640044Z","shell.execute_reply":"2022-12-04T02:35:04.167926Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Bert(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_bilstm=BertLstm(num_class=2,emb_size=768,max_length=150,unit=128)\nbert_bilstm.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:48:53.583837Z","iopub.execute_input":"2022-12-04T02:48:53.584248Z","iopub.status.idle":"2022-12-04T02:48:55.108472Z","shell.execute_reply.started":"2022-12-04T02:48:53.584213Z","shell.execute_reply":"2022-12-04T02:48:55.107206Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"BertLstm(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (bilstm): LSTM(768, 128, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=256, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_textcnn=bert_textcnn(num_filter=128,\n                          filter_sizes=[3,4,5],\n                          in_channels=1,\n                          num_class=2,\n                          emb_size=768,\n                          max_length=150)\nbert_textcnn.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:55:25.599226Z","iopub.execute_input":"2022-12-04T07:55:25.599591Z","iopub.status.idle":"2022-12-04T07:55:27.640230Z","shell.execute_reply.started":"2022-12-04T07:55:25.599559Z","shell.execute_reply":"2022-12-04T07:55:27.639075Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"bert_textcnn(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (textcnn): TextCNN(\n    (convlist): ModuleList(\n      (0): Conv2d(1, 128, kernel_size=(3, 768), stride=(1, 1))\n      (1): Conv2d(1, 128, kernel_size=(4, 768), stride=(1, 1))\n      (2): Conv2d(1, 128, kernel_size=(5, 768), stride=(1, 1))\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=384, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"print(next(textcnn.parameters()).device)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T13:19:00.226096Z","iopub.execute_input":"2022-11-30T13:19:00.226479Z","iopub.status.idle":"2022-11-30T13:19:00.233036Z","shell.execute_reply.started":"2022-11-30T13:19:00.226445Z","shell.execute_reply":"2022-11-30T13:19:00.231193Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"bert_textcnn(['#2019春运广州#//@广州交通:#温馨提示#目前，广州公交集团已在昌岗、公园前已安排应急出租车各30台。深夜乘坐地铁二号线夜间加班线路的旅客，可以在此两站出站换乘出租车。祝您出行顺利！@广州交通电台@FM1052羊城交通台'])","metadata":{"execution":{"iopub.status.busy":"2022-11-30T14:09:26.326881Z","iopub.execute_input":"2022-11-30T14:09:26.327826Z","iopub.status.idle":"2022-11-30T14:09:26.357006Z","shell.execute_reply.started":"2022-11-30T14:09:26.327773Z","shell.execute_reply":"2022-11-30T14:09:26.355894Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.3413, -0.0596]], device='cuda:0', grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class mydataset(torch.utils.data.Dataset):\n    def __init__(self,data):\n        self.texts=data[0]\n        self.labels=list(map(int,data[1]))\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self,idx):\n        return self.texts[idx],self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:26:54.833886Z","iopub.execute_input":"2022-12-04T07:26:54.834710Z","iopub.status.idle":"2022-12-04T07:26:54.841085Z","shell.execute_reply.started":"2022-12-04T07:26:54.834669Z","shell.execute_reply":"2022-12-04T07:26:54.839993Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"dataset=mydataset((train_texts,train_labels))\ndataloader=torch.utils.data.DataLoader(dataset,batch_size=2,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:55:36.384590Z","iopub.execute_input":"2022-12-04T07:55:36.384967Z","iopub.status.idle":"2022-12-04T07:55:36.391015Z","shell.execute_reply.started":"2022-12-04T07:55:36.384929Z","shell.execute_reply":"2022-12-04T07:55:36.390119Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"#分层学习率\noptim_group=[\n    {\"params\":[p for n,p in bert_textcnn.named_parameters() if 'bert' in n],\"lr\":1e-5},\n    {\"params\":[p for n,p in bert_textcnn.named_parameters() if 'bert' not in n],\"lr\":1e-4}\n]","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:37:06.695956Z","iopub.execute_input":"2022-12-04T07:37:06.696704Z","iopub.status.idle":"2022-12-04T07:37:06.703729Z","shell.execute_reply.started":"2022-12-04T07:37:06.696667Z","shell.execute_reply":"2022-12-04T07:37:06.702696Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"#统一学习率\nlf=nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(bert_textcnn.parameters(),lr=1e-5,weight_decay=0.01)\nscheduler=torch.optim.lr_scheduler.StepLR(optimizer=optimizer,step_size=len(dataloader),gamma=0.6)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:55:41.347495Z","iopub.execute_input":"2022-12-04T07:55:41.347851Z","iopub.status.idle":"2022-12-04T07:55:41.357194Z","shell.execute_reply.started":"2022-12-04T07:55:41.347821Z","shell.execute_reply":"2022-12-04T07:55:41.356061Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"#分层学习率\nlf=nn.CrossEntropyLoss()\noptimizer=torch.optim.Adam(optim_group,weight_decay=0.01)\nscheduler=torch.optim.lr_scheduler.StepLR(optimizer=optimizer,step_size=len(dataloader),gamma=0.6)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:37:09.224493Z","iopub.execute_input":"2022-12-04T07:37:09.224846Z","iopub.status.idle":"2022-12-04T07:37:09.231897Z","shell.execute_reply.started":"2022-12-04T07:37:09.224815Z","shell.execute_reply":"2022-12-04T07:37:09.230683Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def notrain(model,test_data,num_pos,num_neg):\n    texts,labels=test_data[0],test_data[1]\n    dataloader=torch.utils.data.DataLoader(mydataset((texts,labels)),batch_size=1,shuffle=False)\n    pos,neg=0,0\n    for i,batch in enumerate(dataloader):\n        text,label=batch[0],batch[1]\n        probas=model(text)\n        pred=torch.argmax(probas,dim=1)\n        label=label.numpy()[0]\n        pred=pred.detach().cpu().numpy()[0]\n        if pred==label:\n            if pred==1:\n                pos+=1\n            if pred==0:\n                neg+=1\n        print(\"\\rprocess:{}/{}\".format(i+1,len(dataloader)),end='')\n    print()\n    print(\"正类的准确率：\",pos/num_pos)\n    print(\"负类的准确率：\",neg/num_neg)\n    print(\"整体的准确率：\",(pos+neg)/len(dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:53:14.210846Z","iopub.execute_input":"2022-12-04T02:53:14.211841Z","iopub.status.idle":"2022-12-04T02:53:14.221291Z","shell.execute_reply.started":"2022-12-04T02:53:14.211800Z","shell.execute_reply":"2022-12-04T02:53:14.220120Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def train(model):\n#     model.train()\n    for epoch in range(5):\n        model.train()#因为每个epoch都会进行验证了，此时模型进入了eval模式，所有需要将train模型放到循环内部\n        print('='*10,\"epoch:{}/{}\".format(epoch+1,5),'='*10)\n        total_loss=0\n        for i,batch in enumerate(dataloader):\n            optimizer.zero_grad()\n            text,label=batch[0],batch[1]\n            label=label.to(device)\n            output=model(text)\n            loss=lf(output,label).to(device)\n            total_loss+=loss.item()\n            loss.backward()\n            optimizer.step()\n            if i%10==0:\n                print(\"\\rprocess:{}/{} | loss={} | lr={}\".format(i+1,len(dataloader),total_loss/10,scheduler.get_lr()[0]),end='')\n                total_loss=0\n            scheduler.step()\n        print()\n        #根据loss保存最优模型，每个epoch结束后都进行一次预测\n        print(\"开始预测...\")\n        predict(epoch+1,model)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:53:01.348990Z","iopub.execute_input":"2022-12-04T07:53:01.349393Z","iopub.status.idle":"2022-12-04T07:53:01.357818Z","shell.execute_reply.started":"2022-12-04T07:53:01.349360Z","shell.execute_reply":"2022-12-04T07:53:01.356616Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def test(model,test_data,num_neg,num_pos,epoch):\n    bad_text,bad_pred,bad_label=[],[],[]\n    texts,labels=test_data[0],test_data[1]\n    dataloader=torch.utils.data.DataLoader(mydataset((texts,labels)),batch_size=1,shuffle=False)\n    model.eval()\n    pos,neg=0,0\n    with torch.no_grad():\n        for i,batch in enumerate(dataloader):\n            text,label=batch[0],batch[1]\n            probas=model(text)\n            pred=torch.argmax(probas,dim=1)\n            label=label.numpy()[0]\n            pred=pred.detach().cpu().numpy()[0]\n            if pred==label:\n                if pred==0:\n                    neg+=1\n                if pred==1:\n                    pos+=1\n            else:\n                #输出bad case\n                bad_text.append(text[0])\n                bad_pred.append(pred)\n                bad_label.append(label)\n            print(\"\\rprocess:{}/{}\".format(i+1,len(dataloader)),end='')\n    print()\n    #保存bad case文件\n    bad_df=pd.DataFrame({'text':bad_text,'label':bad_label,'pred':bad_pred})\n    bad_df.to_csv('./bad_case_{}_5e-6.csv'.format(epoch),index=False)\n    print(\"正类的准确率：\",pos/num_pos)\n    print(\"负类的准确率：\",neg/num_neg)\n    print(\"整体的准确率：\",(pos+neg)/len(dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:53:03.486369Z","iopub.execute_input":"2022-12-04T07:53:03.487152Z","iopub.status.idle":"2022-12-04T07:53:03.497856Z","shell.execute_reply.started":"2022-12-04T07:53:03.487111Z","shell.execute_reply":"2022-12-04T07:53:03.496865Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# def predict(epoch):\n#     num_pos,num_neg=0,0\n#     for i in test_labels:\n#         print(i,type(i))\n#         if i=='0':\n#             num_neg+=1\n#         elif i=='1':\n#             num_pos+=1\n#         else:\n#             print('error')\n#     test(bert_bilstm,(test_texts,test_labels),num_neg,num_pos,epoch)","metadata":{"execution":{"iopub.status.busy":"2022-11-30T06:25:24.015566Z","iopub.execute_input":"2022-11-30T06:25:24.015925Z","iopub.status.idle":"2022-11-30T06:25:24.022293Z","shell.execute_reply.started":"2022-11-30T06:25:24.015893Z","shell.execute_reply":"2022-11-30T06:25:24.021153Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def predict(epoch,model):\n    num_pos,num_neg=0,0\n    for i in test_labels:\n        if i==0:\n            num_neg+=1\n        elif i==1:\n            num_pos+=1\n        else:\n            print('error')\n    test(model,(test_texts,test_labels),num_neg,num_pos,epoch)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:53:06.245669Z","iopub.execute_input":"2022-12-04T07:53:06.246219Z","iopub.status.idle":"2022-12-04T07:53:06.252258Z","shell.execute_reply.started":"2022-12-04T07:53:06.246165Z","shell.execute_reply":"2022-12-04T07:53:06.251232Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# num_pos,num_neg=0,0\n# for i in test_labels:\n#     if i==0:\n#         num_neg+=1\n#     elif i==1:\n#         num_pos+=1\n#     else:\n#         print('error')\n# notrain(bert_textcnn,(test_texts,test_labels),num_pos,num_neg)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T02:53:26.699871Z","iopub.execute_input":"2022-12-04T02:53:26.700247Z","iopub.status.idle":"2022-12-04T02:53:33.100032Z","shell.execute_reply.started":"2022-12-04T02:53:26.700213Z","shell.execute_reply":"2022-12-04T02:53:33.098817Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"process:180/180\n正类的准确率： 0.5384615384615384\n负类的准确率： 0.4473684210526316\n整体的准确率： 0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"model=train(bert_textcnn)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T07:55:46.373757Z","iopub.execute_input":"2022-12-04T07:55:46.374134Z","iopub.status.idle":"2022-12-04T07:57:38.703620Z","shell.execute_reply.started":"2022-12-04T07:55:46.374102Z","shell.execute_reply":"2022-12-04T07:57:38.702565Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"========== epoch:1/5 ==========\nprocess:351/359 | loss=0.38425526693463324 | lr=1e-05\n开始预测...\nprocess:180/180\n正类的准确率： 0.9326923076923077\n负类的准确率： 0.8157894736842105\n整体的准确率： 0.8833333333333333\n========== epoch:2/5 ==========\nprocess:351/359 | loss=0.1167408867739141 | lr=6e-0666\n开始预测...\nprocess:180/180\n正类的准确率： 0.9326923076923077\n负类的准确率： 0.8947368421052632\n整体的准确率： 0.9166666666666666\n========== epoch:3/5 ==========\nprocess:351/359 | loss=0.09991640066727996 | lr=3.6e-066999999996e-06\n开始预测...\nprocess:180/180\n正类的准确率： 0.9615384615384616\n负类的准确率： 0.8289473684210527\n整体的准确率： 0.9055555555555556\n========== epoch:4/5 ==========\nprocess:351/359 | loss=0.012132874061353504 | lr=2.1599999999999996e-06\n开始预测...\nprocess:180/180\n正类的准确率： 0.875\n负类的准确率： 0.868421052631579\n整体的准确率： 0.8722222222222222\n========== epoch:5/5 ==========\nprocess:351/359 | loss=0.08050419825594872 | lr=1.2959999999999997e-066\n开始预测...\nprocess:180/180\n正类的准确率： 0.8846153846153846\n负类的准确率： 0.8289473684210527\n整体的准确率： 0.8611111111111112\n","output_type":"stream"}]},{"cell_type":"markdown","source":"| model | finetuning | batch_size | epoch | init_lr | max_length | unit\\filter_num | filter_size | dropout | data | acc_pos | acc_neg | acc |\n| :----: | :-----: | :----: | :----: | :----: | :-----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| bert | N | - | - | - | - | - | - | - | 重新标注的900条数据 | 0.86 | 0.13 | 0.55 |\n| bert | Y | 16 | 5 | 2e-5 | 128 | - | - | 0.3 | - | 0.82 | 0.92 | 0.88 |\n| bert | Y | 16 | 5 | 1e-5 | 128 | - | - | 0.3 | - |  |  |  |\n| bert | Y | 16 | 5 | 5e-6 | 128 | - | - | 0.3 | - | 0.9 | 0.9 | 0.9 |\n| bert | Y | 4 | 5 | 2e-5 | 128 | - | - | 0.3 | 重新标注的900条数据 | 0.88 | 0.93 | 0.905 |\n| bert | Y | 4 | 5 | 1e-5 | 128 | - | - | 0.3 | 重新标注的900条数据 | 0.91 | 0.92 | 0.916 |\n| bert | Y | 4 | 5 | 5e-6 | 128 | - | - | 0.3 | 重新标注的900条数据 | 0.92 | 0.89 | 0.91 |\n| bert | Y | 4 | 5 | 1e-5/1e-4(分层) | 128 | - | - | 0.3 | 重新标注的900条数据 | 0.89 | 0.94 | 0.916 |\n| bert-bilstm | N | - | - | - | - | - | - | - | 重新标注的900条数据 | 0.71 | 0.23 | 0.51 |\n| bert-bilstm | Y | 16 | 5 | 2e-5 | 128 | 128 | - | 0.2 | - | 0.76 | 0.94 | 0.87 |\n| bert-bilstm | Y | 16 | 5 | 2e-5 | 256 | 128 | - | 0.2 | - | 0.83 | 0.9 | 0.87 |\n| bert-bilstm | Y | 16 | 5 | 2e-5 | 256 | 128 | - | 0.2 | 数据清洗 | 0.81 | 0.87 | 0.84 |\n| bert-bilstm | Y | 16 | 5 | 2e-5 | 128 | 128 | - | 0.3 | 数据清洗 | 0.8 | 0.88 | 0.85 |\n| bert-bilstm | Y | 16 | 5 | 1.2e-5 | 128 | 128 | - | 0.3 | 数据清洗 | 0.88 | 0.9 | 0.9 |\n| bert-bilstm | Y | 16 | 5 | 1e-5 | 150 | 128 | - | 0.3 | 数据清洗 | 0.89 | 0.9 | 0.9 |\n| bert-bilstm | Y | 16 | 5 | 5e-6 | 150 | 128 | - | 0.3 | 数据清洗 | 0.87 | 0.92 | 0.9 |\n| bert-bilstm | Y | 4 | 5 | 5e-6 | 150 | 128 | - | 0.3 | 重新标注的900条数据 | 0.93 | 0.86 | 0.9 |\n| bert-bilstm | Y | 2 | 5 | 1e-5 | 150 | 128 | - | 0.3 | 重新标注的900条数据 | 0.93 | 0.89 | 0.916 |\n| bert-bilstm | Y | 4 | 5 | 1e-5 | 150 | 128 | - | 0.3 | 重新标注的900条数据 | 0.91 | **0.96** | **0.933** |\n| bert-bilstm | Y | 8 | 5 | 1e-5 | 150 | 128 | - | 0.3 | 重新标注的900条数据 | 0.93 | 0.89 | 0.916 |\n| bert-bilstm | Y | 4 | 5 | 2e-5 | 150 | 128 | - | 0.3 | 重新标注的900条数据 | 0.9 | 0.88 | 0.89 |\n| bert-bilstm | Y | 4 | 5 | 1e-5/1e-4(分层) | 150 | 128 | - | 0.3 | 重新标注的900条数据 | 0.92 | 0.93 | 0.927 |\n| bert-textcnn | N | - | - | - | - | - | - | - | 重新标注的900条数据 | 0.53 | 0.44 | 0.5 |\n| bert-textcnn | Y | 2 | 5 | 1e-5 | 150 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.93 | 0.89 | 0.916 |\n| bert-textcnn | Y | 4 | 5 | 1e-5 | 150 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.93 | 0.92 | 0.927 |\n| bert-textcnn | Y | 8 | 5 | 1e-5 | 150 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.93 | 0.89 | 0.916 |\n| bert-textcnn | Y | 4 | 5 | 1e-5 | 150 | 128 | [4,5,6] | 0.3 | 重新标注的900条数据 | 0.92 | 0.92 | 0.92 |\n| bert-textcnn | Y | 4 | 5 | 1e-5 | 150 | 100 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.9 | 0.93 | 0.916 |\n| bert-textcnn | Y | 4 | 5 | 1e-5 | 128 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.92 | 0.92 | 0.92 |\n| bert-textcnn | Y | 4 | 5 | 2e-5 | 150 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | **0.96** | 0.84 | 0.91 |\n| bert-textcnn | Y | 4 | 5 | 2e-5 | 128 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.903 | 0.907 | 0.905 |\n| bert-textcnn | Y | 2 | 5 | 1e-5/1e-4(分层) | 128 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.89 | 0.89 | 0.89 |\n| bert-textcnn | Y | 4 | 5 | 1e-5/1e-4(分层) | 128 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | **0.96** | 0.89 | **0.933** |\n| bert-textcnn | Y | 8 | 5 | 1e-5/1e-4(分层) | 128 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.93 | 0.92 | 0.927 |\n| bert-textcnn | Y | 4 | 5 | 1e-5/1e-4(分层) | 150 | 128 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.93 | 0.9 | 0.92 |\n| bert-textcnn | Y | 4 | 5 | 1e-5/1e-4(分层) | 150 | 128 | [4,5,6] | 0.3 | 重新标注的900条数据 | 0.92 | 0.85 | 0.89 |\n| bert-textcnn | Y | 4 | 5 | 1e-5/1e-4(分层) | 150 | 100 | [3,4,5] | 0.3 | 重新标注的900条数据 | 0.95 | 0.828 | 0.9 |","metadata":{}},{"cell_type":"markdown","source":"bert\n\n- batch_size=16\n- epoch=5\n- lr=2e-5\n- max_length=128\n- dropout=0.3\n- 结果:\n  - 正类的准确率： 0.8203125\n  - 负类的准确率： 0.9186046511627907\n  - 整体的准确率： 0.8809808612440191\n  \n- batch_size=16\n- epoch=5\n- lr=5e-6\n- max_length=128\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9038461538461539\n  - 负类的准确率： 0.9078947368421053\n  - 整体的准确率： 0.9055555555555556\n  \n- batch_size=4\n- epoch=5\n- lr=2e-5\n- max_length=128\n- dropout=0.3\n- 数据：900条\n- 结果：\n  - 正类的准确率： 0.8846153846153846\n  - 负类的准确率： 0.9342105263157895\n  - 整体的准确率： 0.9055555555555556\n  \n- batch_size=4\n- epoch=5\n- lr=1e-5\n- max_length=128\n- dropout=0.3\n- 数据：900条\n- 结果：\n  - 正类的准确率： 0.9134615384615384\n  - 负类的准确率： 0.9210526315789473\n  - 整体的准确率： 0.9166666666666666\n  \n- batch_size=4\n- epoch=5\n- lr=5e-6\n- max_length=128\n- dropout=0.3\n- 数据：900条\n- 结果：\n  - 正类的准确率： 0.9230769230769231\n  - 负类的准确率： 0.8947368421052632\n  - 整体的准确率： 0.9111111111111111\n  \n- batch_size=4\n- epoch=5\n- lr=1e-5/1e-4\n- max_length=128\n- dropout=0.3\n- 数据：900条\n- 结果：\n  - 正类的准确率： 0.8942307692307693\n  - 负类的准确率： 0.9473684210526315\n  - 整体的准确率： 0.9166666666666666\n---\n\nbert-bilstm\n\n- 无训练数据\n- 注：使用同样的900条数据，对0.2的数据进行预测\n- 结果\n  - 正类的准确率： 0.7115384615384616\n  - 负类的准确率： 0.23684210526315788\n  - 整体的准确率： 0.5111111111111111\n\n- batch_size=16\n- epoch=5\n- lr=2e-5\n- max_length=128\n- unit=128\n- dropout=0.2\n- 结果：\n  - 正类的准确率： 0.765625\n  - 负类的准确率： 0.9457364341085271\n  - 整体的准确率： 0.8767942583732058\n  \n\n- batch_size=16\n- epoch=5\n- lr=2e-5\n- max_length=256\n- unit=128\n- dropout=0.2\n- 结果：\n  - 正类的准确率： 0.828125\n  - 负类的准确率： 0.8992248062015504\n  - 整体的准确率： 0.8720095693779905\n  \n- batch_size=16\n- epoch=5\n- lr=2e-5\n- max_length=256\n- unit=128\n- dropout=0.2\n- 去除@#url重复\n- 结果：\n  - 正类的准确率： 0.8125\n  - 负类的准确率： 0.8682170542635659\n  - 整体的准确率： 0.84688995215311\n  \n- batch_size=16\n- epoch=5\n- lr=2e-5\n- max_length=128\n- unit=128\n- dropout=0.3\n- 去除@#url重复html\n- 结果：\n  - 正类的准确率： 0.8046875\n  - 负类的准确率： 0.8827519379844961\n  - 整体的准确率： 0.8528708133971292\n \n- batch_size=16\n- epoch=5\n- lr=1.2e-5\n- max_length=128\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 结果：\n  - 正类的准确率： 0.8817034700315457\n  - 负类的准确率： 0.9104186952288218\n  - 整体的准确率： 0.8994581577363034\n  \n- batch_size=16\n- epoch=5\n- lr=1e-5\n- max_length=150\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 注：好像没有重新创建模型\n- 结果：\n  - 正类的准确率： 0.8974763406940063\n  - 负类的准确率： 0.9065238558909445\n  - 整体的准确率： 0.9030704394942806\n\n- batch_size=16\n- epoch=5\n- lr=5e-6\n- max_length=150\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 注：好像没有重新创建模型，只有一个的结果比较高\n- 结果：\n  - 正类的准确率： 0.8769716088328076\n  - 负类的准确率： 0.9211295034079844\n  - 整体的准确率： 0.9042745334136063\n  \n- batch_size=4\n- epoch=5\n- lr=5e-6\n- max_length=150\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 注：使用的是新标的900条数据\n- 结果：\n  - 正类的准确率： 0.9326923076923077\n  - 负类的准确率： 0.868421052631579\n  - 整体的准确率： 0.9055555555555556\n  \n- batch_size=4\n- epoch=5\n- lr=1e-5\n- max_length=150\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 注：使用的是新标的900条数据\n- 结果：\n  - 正类的准确率： 0.9134615384615384\n  - 负类的准确率： 0.9605263157894737\n  - 整体的准确率： 0.9333333333333333\n\n- batch_size=4\n- epoch=5\n- lr=2e-5\n- max_length=150\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 注：使用的是新标的900条数据\n- 结果：\n  - 正类的准确率： 0.9038461538461539\n  - 负类的准确率： 0.881578947368421\n  - 整体的准确率： 0.8944444444444445\n\n- batch_size=4\n- epoch=5\n- lr=1e-5/1e-4\n- max_length=150\n- unit=128\n- dropout=0.3\n- 去除@#url重复html空白行\n- 注：使用的是新标的900条数据\n- 结果：\n  - 正类的准确率： 0.9230769230769231\n  - 负类的准确率： 0.9342105263157895\n  - 整体的准确率： 0.9277777777777778\n---\nbert_textcnn\n\n- batch_size=4\n- epoch=5\n- lr=1e-5\n- max_length=150\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9326923076923077\n  - 负类的准确率： 0.9210526315789473\n  - 整体的准确率： 0.9277777777777778\n  \n- batch_size=4\n- epoch=5\n- lr=2e-5\n- max_length=150\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9615384615384616\n  - 负类的准确率： 0.8421052631578947\n  - 整体的准确率： 0.9111111111111111\n  \n- batch_size=4\n- epoch=5\n- lr=5e-6\n- max_length=150\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9230769230769231\n  - 负类的准确率： 0.868421052631579\n  - 整体的准确率： 0.9\n  \n- batch_size=4\n- epoch=5\n- 分层学习率：lr=1e-5/1e-4\n- max_length=150\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9326923076923077\n  - 负类的准确率： 0.9078947368421053\n  - 整体的准确率： 0.9222222222222223\n  \n- batch_size=4\n- epoch=5\n- lr=1e-5\n- max_length=150\n- filter_sizes=[4,5,6]\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9230769230769231\n  - 负类的准确率： 0.9210526315789473\n  - 整体的准确率： 0.9222222222222223\n  \n- batch_size=4\n- epoch=5\n- lr=1e-5\n- max_length=150\n- filter_sizes=[4,5,6]\n- dropout=0.3\n- num_filter=100\n- 结果：\n  - 正类的准确率： 0.9038461538461539\n  - 负类的准确率： 0.9342105263157895\n  - 整体的准确率： 0.9166666666666666\n  \n- batch_size=4\n- epoch=5\n- 分层学习率：lr=1e-5/1e-4\n- max_length=150\n- filter_sizes=[4,5,6]\n- dropout=0.3\n- 结果：\n  - 正类的准确率： 0.9230769230769231\n  - 负类的准确率： 0.8552631578947368\n  - 整体的准确率： 0.8944444444444445\n\n- batch_size=4\n- epoch=5\n- 分层学习率：lr=1e-5/1e-4\n- max_length=150\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- num_filter=100\n- 结果：\n  - 正类的准确率： 0.9519230769230769\n  - 负类的准确率： 0.8289473684210527\n  - 整体的准确率： 0.9\n  \n- batch_size=4\n- epoch=5\n- lr=1e-5\n- max_length=128\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- num_filter=100\n- 结果：\n  - 正类的准确率： 0.9230769230769231\n  - 负类的准确率： 0.9210526315789473\n  - 整体的准确率： 0.9222222222222223\n\n\n- batch_size=4\n- epoch=5\n- lr=2e-5\n- max_length=128\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- num_filter=100\n- 结果：\n  - 正类的准确率： 0.9038461538461539\n  - 负类的准确率： 0.9078947368421053\n  - 整体的准确率： 0.9055555555555556\n\n\n- batch_size=4\n- epoch=5\n- 分层学习率：lr=1e-5/1e-4\n- max_length=128\n- filter_sizes=[3,4,5]\n- dropout=0.3\n- num_filter=100\n- 结果：\n  - 正类的准确率： 0.9615384615384616\n  - 负类的准确率： 0.8947368421052632\n  - 整体的准确率： 0.9333333333333333","metadata":{}}]}